{"metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "version": "3.6.3", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"metadata": {"_cell_guid": "b44bf30c-f612-48ea-a8b5-0ddf1995dac7", "_uuid": "a7ddd56c08a783e2494fdfe663ce90f50eb1a646"}, "cell_type": "markdown", "source": ["# Exemplos do livro NLTK com textos em portugu\u00eas\n", "\n", "Fiz apenas algumas pequenas modifica\u00e7\u00f5es, aqui e ali, ao longo no notebook, s\u00f3 para ficar um pouco mais interessante e f\u00e1cil de companhar tamb\u00e9m. Mas o conte\u00fado permanece fiel aos exerc\u00edcios do livro."]}, {"metadata": {"_cell_guid": "a0d08399-b8c3-4af9-b096-8a9f5421ebc9", "_uuid": "b9ba009695aa1750c0abda5dede49526c0441364"}, "cell_type": "markdown", "source": ["# 1. Set up dos textos e senten\u00e7as"]}, {"metadata": {"_cell_guid": "c4aa55b0-acb4-486f-a9cd-85188f2fcfee", "_uuid": "ce171d935e0eb229f100fd6ea89ed0fd2084e261"}, "cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "# Natural Language Toolkit: Some Portuguese texts for exploration in chapter 1 of the book\n", "#\n", "# Copyright (C) 2001-2014 NLTK Project\n", "# Author: Steven Bird <stevenbird1@gmail.com>\n", "# URL: <http://nltk.org/>\n", "# For license information, see LICENSE.TXT\n", "from __future__ import print_function, unicode_literals\n", "\n", "from nltk.corpus import machado, mac_morpho, floresta, genesis\n", "from nltk.text import Text\n", "from nltk.probability import FreqDist\n", "from nltk.util import bigrams\n", "from nltk.misc import babelize_shell\n", "\n", "print(\"*** Exemplos introdut\u00f3rios do livro NLTK em Portugu\u00eas ***\")\n", "print(\"Carregando ptext1...ptext4 e psent1...ptext4\")\n", "print(\"Use 'texts()' ou 'sents()' para listar os materiais de texto ou senten\u00e7a.\")\n", "\n", "ptext1 = Text(machado.words('romance/marm05.txt'), name=\"Mem\u00f3rias P\u00f3stumas de Br\u00e1s Cubas (1881)\")\n", "ptext2 = Text(machado.words('romance/marm08.txt'), name=\"Dom Casmurro (1899)\")\n", "ptext3 = Text(genesis.words('portuguese.txt'), name=\"G\u00eanesis\")\n", "ptext4 = Text(mac_morpho.words('mu94se01.txt'), name=\"Folha de Sao Paulo (1994)\")\n", "\n", "def texts():\n", "    print(\"ptext1:\", ptext1.name)\n", "    print(\"ptext2:\", ptext2.name)\n", "    print(\"ptext3:\", ptext3.name)\n", "    print(\"ptext4:\", ptext4.name)\n", "\n", "psent1 = \"o amor da gl\u00f3ria era a coisa mais verdadeiramente humana que h\u00e1 no homem , e , conseq\u00fcentemente , a sua mais genu\u00edna fei\u00e7\u00e3o .\".split()\n", "psent2 = \"N\u00e3o consultes dicion\u00e1rios .\".split()\n", "psent3 = \"No princ\u00edpio, criou Deus os c\u00e9us e a terra.\".split()\n", "psent4 = \"A C\u00e1ritas acredita que outros cubanos devem chegar ao Brasil .\".split()\n", "\n", "def sents():\n", "    print(\"psent1:\", \" \".join(psent1))\n", "    print(\"psent2:\", \" \".join(psent2))\n", "    print(\"psent3:\", \" \".join(psent3))\n", "    print(\"psent4:\", \" \".join(psent4))"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "aa4235b7-ae4e-4833-a73b-d1cf8ce99f2c", "_kg_hide-input": false, "_uuid": "aa98e6ae145b979ce7d058e986cbee61ab11a651"}, "cell_type": "code", "source": ["# Vejamos ent\u00e3o um de nossos textos\n", "ptext2"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "3ccf3ede-7b7b-47b6-ac3a-7175f879da01", "_uuid": "0d0c4de380aed287ddb8d2bea193ecbf7bb8bf5d"}, "cell_type": "code", "source": ["# E uma das senten\u00e7a\n", "psent3"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "736ae370-23a7-4e38-a5f0-b73013347d78", "_uuid": "3028a224ec838b3c05ed2cf395748ef1b3a780cb"}, "cell_type": "code", "source": ["# Agora, todos os textos\n", "texts()"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "1d518e3a-29c5-4673-93a7-fc27d3b77279", "_uuid": "428ea501701bbf360881e19463ae7062e040fa9b"}, "cell_type": "code", "source": ["# E todas as senten\u00e7as\n", "sents()"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "e27681a0-24ba-4cf0-badb-be767ce6ea13", "_uuid": "5dd5a991a67a363cb40a70b30d14fd7c4a0d4bc1"}, "cell_type": "markdown", "source": ["## 1.1. Buscando por Textos"]}, {"metadata": {"_cell_guid": "0c26fc25-0ece-4fc4-850a-750e312a05ee", "_uuid": "a900888b30f5e215469d4c2e27d80b9496da64d2"}, "cell_type": "code", "source": ["# O m\u00e9todo concordance permite ver palavras em um contexto\n", "ptext1.concordance('olhos')"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "6a0854d4-7a89-438f-b8ec-5d3a9cf4cdaf", "_uuid": "633d64b0051b167c2a9a23bd0f8e8e8f174cba55"}, "cell_type": "code", "source": ["# Para uma dada palavras, \u00e9 poss\u00edvel encontrar palavras com distribui\u00e7\u00e3o de texto similar\n", "print(f\"{ptext1.name}:\")\n", "ptext1.similar('chegar')\n", "print(f\"\\n{ptext3.name}:\")\n", "ptext3.similar('chegar')"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "00430bf5-4305-43ec-a90e-5f2a82b35527", "_uuid": "9ec942b778004cc7d9e8295df525ea3e35c49610"}, "cell_type": "code", "source": ["# \u00c9 poss\u00edvel procurar no texto por coloca\u00e7\u00f5es significativamente significantes\n", "ptext1.collocations()"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "76d53dcd-9c6e-498d-a6c2-f60d7e332f8e", "_uuid": "67ab7f3962608d3f050ddd03440244d1d414fb74"}, "cell_type": "code", "source": ["# Express\u00f5es regulares tamb\u00e9m podem ser usadas para procurar palavras em contexto\n", "ptext1.findall(\"<olhos> (<.*>)\")"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "dbf9d26e-dd47-4123-b6e4-fa22381362e4", "collapsed": true, "_uuid": "59814ad0c39b657924b2205f06ba0341d6a6abec"}, "cell_type": "code", "source": ["# Gerar texto randomicamente baseado em um dado texto? Sim, d\u00e1 para fazer isso. Digo, n\u00e3o. Esquece isso\n", "ptext1.generate(words=None)"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "1bf9af03-fee0-49be-9fb6-7fd1ba6b8cbd", "_uuid": "62a99aaea9e649678c32a7968d0b7b282af23a5b"}, "cell_type": "markdown", "source": ["## 1.2. Textos na forma de Listas de Palavras"]}, {"metadata": {"_cell_guid": "b6605364-8f18-4471-9c5e-3894e447add2", "_uuid": "1ec880debb0bfafebef2ccad9fc0851371ce5af7"}, "cell_type": "code", "source": ["# Como vimos nos primeiro passos, algumas senten\u00e7as j\u00e1 foram definidas\n", "sents()"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "69db3b5f-c7ca-415d-9057-2c4180e44922", "_uuid": "2a765a341872b72e6114b5e55f95667469175d06"}, "cell_type": "code", "source": ["psent1"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "4ec3076c-d7ac-4d42-84b0-3ec72553d282", "_uuid": "758c53e951576393d3aa2ed13d42f47ad886f9c6"}, "cell_type": "code", "source": ["# Perceba que a senten\u00e7a foi tokenizada. Cada token \u00e9 representado como uma string, inclusive, a\n", "# pontua\u00e7\u00e3o, a.k.a \"ponto\" e \"v\u00edrgula\". Perceba ent\u00e3o que os token s\u00e3o combinados em uma lista,\n", "# cujo tamanho \u00e9...\n", "len(psent1)"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "e9930713-3676-459c-beba-b8652954a88c", "_uuid": "3a7cea55d5c05914717992c77b885a0f50e45e8c"}, "cell_type": "code", "source": ["# Vejamos qual o vocabul\u00e1rio dessa senten\u00e7a\n", "sorted(set(psent1))"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "d768a9d2-e07b-4676-90dd-2ccb4e4358ac", "_uuid": "e8e7b20f8a4d1ea18c8bde3d5ecf9a7bac1de666"}, "cell_type": "code", "source": ["# Para cada palavra do vocabul\u00e1rio da senten\u00e7a, vamos imprimir algumas informa\u00e7\u00f5es b\u00e1sicas. Mas dessa\n", "# vez, vamos excluir a pontua\u00e7\u00e3o\n", "pontuacao = [\".\", \",\", \";\", \"-\", \":\"]\n", "vocabulario = [v for v in sorted(set(psent1)) if not v in pontuacao]\n", "for w in vocabulario:\n", "    print(f\"{w} = tamanho: {len(w)}, \u00faltimo caractere: {w[-1]}\")"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "07d51bc7-7bf8-4e14-97d1-e15cc13dedb8", "_uuid": "bbaefd0f0223c4a7891fce8233f2db6da17ea174"}, "cell_type": "code", "source": ["# OK, que tal brincar um pouco mais com list comprehension?\n", "tudo_maiusculo = [w.upper() for w in psent2]\n", "print(f\"tudo mai\u00fasculo: {tudo_maiusculo}\\n\")\n", "\n", "palavras_terminadas_em_a = [w for w in psent1 if w.endswith('a')]\n", "print(f\"palavras terminadas em a: {palavras_terminadas_em_a}\\n\")\n", "\n", "palavas_com_mais_de_15_caracteres = [w for w in ptext4 if len(w) > 15]\n", "print(f\"palavas com mais de 15 caracteres: {palavas_com_mais_de_15_caracteres}\")"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "e14d5bd8-0085-441f-9d0f-2b93177bad70", "_uuid": "4f76a0843448ee03846e1676530c62fc15570f5c"}, "cell_type": "code", "source": ["# Podemos inspecionar a frequ\u00eancias em que as palavras aparecem no texto\n", "fd1 = FreqDist(ptext1)\n", "print(f\"{fd1}\")\n", "fd1"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "180801bb-e21a-4281-a772-76547f214250", "_uuid": "3e912fe037404669a3dc71d58d3797eb44671f71"}, "cell_type": "code", "source": ["# OK, vamos ver algumas palavras em especifico\n", "fd1[\"olhos\"]"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "fa0d99ab-645c-4469-acaa-744e252b2d15", "_uuid": "d7361fc636a025fe3369fef3cbfbbb503d06f647"}, "cell_type": "code", "source": ["# Qual a palavra que mais aparece no texto? Advinha...\n", "fd1.max()"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "de0f8460-924b-43fd-8cb9-5b32b284aa00", "collapsed": true, "_uuid": "413c819d1723ce0957bbc464162e2139dd1eedae"}, "cell_type": "code", "source": ["# N\u00e3o tem o m\u00e9todo samples() na classe FreqDist"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "bce9109b-a679-4bbd-bc81-0af455e17537", "_uuid": "51579d2349db7a55a985ee6bae0056c4af83cf9b"}, "cell_type": "markdown", "source": ["# 2. Leando a Corpora\n", "## 2.1. Acessando o corpus de Machado de Assis"]}, {"metadata": {"_cell_guid": "69951ca2-aa89-4d22-96fa-6755713f89a5", "_uuid": "88cf0749b5eb4f3f29de6204aa82cfc9d7df7418"}, "cell_type": "code", "source": ["# NLTK tem os trabalhos completos de Machado Assis, sendo que cada arquivo cont\u00e9m um de seus trabalhos\n", "from nltk.corpus import machado\n", "machado.fileids()"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "2d8ae005-7398-45b3-8e42-655e3c71d1c6", "_uuid": "f7b7e0e698159f0d8527a85b19bef21c5002eb70"}, "cell_type": "code", "source": ["# Voc\u00ea pode ter mais informa\u00e7\u00f5es dando uma lida no readme do corpus\n", "print(machado.readme())"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "5e48ef70-9183-4ce2-a772-9cde19ebbab2", "_uuid": "4e50162b3c237c0587af7eb8b288802748004d0a"}, "cell_type": "code", "source": ["# Vamos dar uma olhada em Mem\u00f3rias P\u00f3stumas de Br\u00e1s Cubas.\n", "# \u00c9 poss\u00edvel acessar o texto como uma lista de caracteres. Por exemplo, vejamos os 200 caracteres a\n", "# partir da posi\u00e7\u00e3o 10000.\n", "raw_text = machado.raw('romance/marm05.txt')\n", "raw_text[10000:10200]"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "38d769e6-8b89-4dbe-b95f-ff03bab794ad", "_uuid": "d0d6a4575529887d8ea1310a067115d5be55aebe"}, "cell_type": "code", "source": ["# No entanto, essa n\u00e3o \u00e9 uma maneira muito \u00fatil de trabalhar com texto. Pensamos nos textos, geralmente,\n", "# como uma sequencia de palavras e pontua\u00e7\u00f5es, n\u00e3o de caracteres\n", "text1 = machado.words('romance/marm05.txt')\n", "text1"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "47411ae3-16d1-4a18-b71f-ba731fbc52cd", "_uuid": "258e9f6533145ba9d02006bda2e572a492a4652d"}, "cell_type": "code", "source": ["# Quantidade de palavras\n", "print(f\"quantidade de palavras: {len(text1)}\")\n", "\n", "# Quantidade de palavras, excluindo pontua\u00e7\u00e3o\n", "print(f\"quantidade de palavras excluindo pontua\u00e7\u00e3o: {len([p for p in text1 if not p in pontuacao])}\")\n", "\n", "# Quantidade de palavras \u00fanicas\n", "print(f\"quantidade de palavras \u00fanicas: {len(set(text1))}\")\n", "\n", "# Quantidade de palavras \u00fanicas, excluindo pontua\u00e7\u00e3o\n", "print(f\"quantidade de palavras \u00fanicas excluindo pontua\u00e7\u00e3o: {len([p for p in set(text1) if not p in pontuacao])}\")"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "f6f099e8-8fc7-4dbb-8633-94954de5cc27", "_uuid": "d1a4250a2bf0aaf3e54fe3ebef089f39d298e661"}, "cell_type": "code", "source": ["# Procurando os ngrams mais comuns, que contenham uma palavra alvo em particular\n", "# NOTA: O livro usa a fun\u00e7\u00e3o ingrams, mas no NLTK 3, ela tornou-se apenas ngram\n", "from nltk import ngrams, FreqDist\n", "target_word = 'olhos'\n", "fd = FreqDist(ng for ng in ngrams(text1, 5) if target_word in ng)\n", "fd"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "055ccbd1-68d8-418e-bdb3-ef71e3f5ace4", "_uuid": "88252d12488312523f86e4a87d514fe42a3d0452"}, "cell_type": "markdown", "source": ["## 2.2. Acessando, Tagged Corpus do MacMorpho\n", "NLTK inclui o texto de not\u00edcias MAC-MORPHO POS-taggeado do Portugu\u00eas Brasileiro, com mais de 1 milh\u00e3o de palavras de textos jornalisticos extra\u00eddos de 10 se\u00e7\u00f5es do jornal di\u00e1rio Folha de S\u00e3o Paulo, 1994."]}, {"metadata": {"_cell_guid": "5873eaa9-b37d-49dd-8137-afe53eed2393", "_uuid": "654db374c598475f7853a0ccb0d448792715ef5f"}, "cell_type": "code", "source": ["# Vejamos uma sequencia de palavras:\n", "from nltk.corpus import mac_morpho\n", "mac_morpho.words()"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "2de5c7de-f1f8-44e7-91c1-8266d1b7f081", "_uuid": "58a057f7c93c97f7f3e212937bf4a0ec2b1a773e"}, "cell_type": "code", "source": ["# Suas senten\u00e7as\n", "mac_morpho.sents()"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "c5c771a7-074a-4a0b-a518-f4dd375d9d7b", "_uuid": "b872e4c84cba931ca4b3bda8cd920e2281ed0f18"}, "cell_type": "code", "source": ["# Suas palavras taggeadas\n", "mac_morpho.tagged_words()"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "04b7e0ff-2112-401d-ac17-ff0c554769cc", "_uuid": "73099b9e5a92fa09d9b1633ee6094c973cd6a050"}, "cell_type": "code", "source": ["# Tamb\u00e9m \u00e9 poss\u00edvel acessar em forma de seten\u00e7as com suas palavras taggeadas\n", "mac_morpho.tagged_sents()"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "ead44882-704c-4360-a59e-fe9bb1cd4a63", "_uuid": "a2d4491b4e27877b9877ceb635d5d050cab05242"}, "cell_type": "code", "source": ["# B\u00f4nus: Taggeamento part-of-speeach (POS)\n", "import nltk\n", "nltk.pos_tag(mac_morpho.sents()[0])"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "298868c0-b335-4ef3-8464-5f8ed9a3f7a9", "_uuid": "7e6975e91132c6ee00b64e6dedffeedf166b2ec0"}, "cell_type": "code", "source": ["# B\u00f4nus: Quer saber o que significa cada tag?\n", "palavra_pos_taggeado = nltk.pos_tag(mac_morpho.sents()[0])\n", "tags = set([tag for (texto, tag) in palavra_pos_taggeado])\n", "for tag in tags:\n", "    nltk.help.upenn_tagset(tag)"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "9e8efab1-9f5e-4e8c-8342-4718d94df59a", "_uuid": "372c548bb086abd8e287917856d46a1866aefe1e"}, "cell_type": "markdown", "source": ["## 2.3. Acessando o Floresta Treebank Portugu\u00eas\n", "As fun\u00e7\u00f5es vista na se\u00e7\u00e3o anterior podem ser usadas para treinar taggers, como no exemplo que vamos ver, que \u00e9 de um Floresta treebank.\n", "\n", "A distribui\u00e7\u00e3o do NLTK inclui o \"Floresta Sinta(c)tica Corpus\" (http://www.linguateca.pt)."]}, {"metadata": {"_cell_guid": "a23b9ea1-414d-41ef-939e-cb5f80555749", "_uuid": "cf840bca1adf112ac911f35bbd1ad0107ad02b1e"}, "cell_type": "code", "source": ["# Podemos acessar o corpus como uma senten\u00e7a de palavras ou palavras taggeadas\n", "from nltk.corpus import floresta\n", "floresta.words()"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "21e6bf6a-46a0-4a98-9231-57475839abf3", "_uuid": "779d4e9e905c4d2a1428317ea51be7844600a145"}, "cell_type": "code", "source": ["# Palavras taggeadas\n", "floresta.tagged_words()"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "b78d37c0-f5ac-42c0-a2f3-6c6648a25cfe", "_uuid": "e0e1caddd89fc1a6efb5ec01d31637d6c5a895b4"}, "cell_type": "code", "source": ["# As tags consistem em alguma informa\u00e7\u00e3o sintatica, seguida por um sinal \"+\", seguido por uma tag\n", "# convencional de part-of-speech (a.k.a grammatical tagging, a.k.a. an\u00e1lise/classifica\u00e7\u00e3o gramatical,\n", "# a.k.a. noun, verb, article, adjective, preposition, pronoun, adverb, conjunction, and interjection,\n", "# a.k.a. substantivo, verbo, artigo, adjetivo, preposi\u00e7\u00e3o, pronome, adverbo, conju\u00e7\u00e3o e interjei\u00e7\u00e3o)\n", "\n", "# OK, vamos cortar fora a parte antes do sinal de adi\u00e7\u00e3o\n", "def simplify_tag(t):\n", "    if \"+\" in t:\n", "        return t[t.index(\"+\") + 1:]\n", "    else:\n", "        return t\n", "\n", "twords = floresta.tagged_words()\n", "twords = [(w.lower(), simplify_tag(t)) for (w, t) in twords]\n", "twords[:10]"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "e2bc7405-39d6-44d2-b066-526440dcba3f", "_uuid": "890f755461ba817828e99644282c9065512c9ba2"}, "cell_type": "code", "source": ["# Pretty print?\n", "print('\\n'.join(word + ' = ' + tag for (word, tag) in twords[:10]))"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "2ac58a1f-e08d-4ec4-93f2-4d70e65f7113", "_uuid": "30460a1fcd866f7fcb6b14cfae6d9ad9c48db538"}, "cell_type": "code", "source": ["# Vamos contar a quantidade de tokens e tipos\n", "words = floresta.words()\n", "print(f\"quantidade de palavras: {len(words)}\")\n", "\n", "# Inspecionar a distribui\u00e7\u00e3o de palavras\n", "fd = nltk.FreqDist(words)\n", "print(f\"distribui\u00e7\u00e3o de palavras: {len(fd)}\")\n", "\n", "# A palavra mais frequente\n", "print(f\"palavra mais frequente: {fd.max()}\")"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "560255d4-1313-4b06-88aa-cabc8a39a156", "_uuid": "6eca07a01deb9b70aa8f9f6528d4509b85e0dc85"}, "cell_type": "code", "source": ["# Quais s\u00e3o as 20 tags mais comuns, em ordem decrescente de frequencia?\n", "tags = [simplify_tag(tag) for (word,tag) in floresta.tagged_words()]\n", "fd = nltk.FreqDist(tags)\n", "fd_keys = [x for x in fd.keys()]\n", "fd_keys[:20]"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "1f8fa9f6-8ac6-43d4-95ab-ab3b5ea513eb", "_uuid": "b9184e71b2b42232233eb8519612d7ca4faa39cd"}, "cell_type": "code", "source": ["# \u00c9 poss\u00edvel tamb\u00e9m acessar o corpus agrupado por senten\u00e7a\n", "floresta.sents()"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "6f70eca7-22c9-4159-8c89-5a438286438a", "_uuid": "eff7253d57ec96a814bb628f2f4cf6d01b608ae6"}, "cell_type": "code", "source": ["# D\u00e1 para fazer isso com a senten\u00e7a taggeada\n", "floresta.tagged_sents()"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "fe1a99f7-52fd-40a6-b756-dded8cc816e8", "_uuid": "9dc03778b41b4f7fbe836a9908f136cc54a610f4"}, "cell_type": "code", "source": ["# Senten\u00e7a parseada, como uma \u00e1rvore\n", "floresta.parsed_sents()"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "c422fd97-1a88-4ed6-96f4-06290aea2df9", "_uuid": "ea1ae3d099e8c806714f05494fb1b645b02c7a3c"}, "cell_type": "code", "source": ["# Vamos ver isso mais bonitinho -- Meh. It fails so bad\n", "from IPython.core.display import display\n", "psents = floresta.parsed_sents()\n", "display(psents)"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "b0154ad1-46de-4b43-9949-0111d2145bbb", "_uuid": "b9cf0e1d46e14b5c19719d73f375dcb89be941f8"}, "cell_type": "markdown", "source": ["## 2.4. Encoding de Caracteres\n", "Pulei essa parte porque n\u00e3o achei muito relevante, rodando Python 3.6. Mas, de qualquer forma, inclui um teste com texto das Mem\u00f3rias P\u00f3stumas de Br\u00e1s Cubas, s\u00f3 porque quando adolescente tive que ir ao teatro ver essa pe\u00e7a com a escola."]}, {"metadata": {"_cell_guid": "d7ab5e93-45d7-4f4f-8bbb-ad0b60668c5a", "_uuid": "37edcce0efaf55a102d080f14052349f9b9a02d2"}, "cell_type": "code", "source": ["raw_text = machado.raw('romance/marm05.txt')\n", "raw_text[:200]"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "55e5455c-cc3f-4ee8-a3ba-d18d33e08671", "_uuid": "9036326a508cab8fcc8f7494377005ed61f9eabc"}, "cell_type": "markdown", "source": ["# 3. Processamento de Tarefas\n", "## 3.1. Concord\u00e2ncia Simples"]}, {"metadata": {"_cell_guid": "fe0847a7-a8f4-4350-9613-b17461873c59", "_uuid": "dbc3b385491d17802d7970ba5388e2d5ef51898b"}, "cell_type": "code", "source": ["# Vamos criar uma fun\u00e7\u00e3o que, dado uma palavra e um tamanho de contexto (em n\u00famero de caracteres),\n", "# gere a concord\u00e2ncia da tal palavra dentro de um conjunto de senten\u00e7as\n", "def concordance(word, context=30):\n", "    print(f\"palavra: {word}, contexto: {context} caracteres\")\n", "    for sent in floresta.sents():\n", "        if word in sent:\n", "            pos = sent.index(word)\n", "            left = \" \".join(sent[:pos])\n", "            right = \" \".join(sent[pos + 1:])\n", "            print(f\"{left[-context:]} '{word}' {right[:context]}\")\n", "\n", "concordance(\"dar\")"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "86fba418-c7f3-46af-afdf-92088247c78f", "_uuid": "24464a0f19e283243679214793643f3946a1c1a4"}, "cell_type": "code", "source": ["# Mais uma? Que tal vender?\n", "concordance(\"vender\")"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "738d8b97-4aa4-49df-b9ec-9d51ed16fb2f", "_uuid": "27f33a6d7a694e21a1e45b094ac3c0efcea430fd"}, "cell_type": "markdown", "source": ["## 3.2. Part-of-Speech Tagging, a.k.a. An\u00e1lise/Classifica\u00e7\u00e3o Gramatical"]}, {"metadata": {"_cell_guid": "6cc95a4d-e9fa-4a3b-b55d-b88b528d6085", "collapsed": true, "_uuid": "9a4de8486e3f1e12111330a0b176576f3201f890"}, "cell_type": "code", "source": ["# OK, vamos primeiro pegar as tags das senten\u00e7as e simplific\u00e1-las, como j\u00e1 fizemos anteriormente\n", "from nltk.corpus import floresta\n", "tsents = floresta.tagged_sents()\n", "tsents = [[(w.lower(),simplify_tag(t)) for (w,t) in sent] for sent in tsents if sent]"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "727e210a-eebc-4921-9671-64111869e9c7", "_uuid": "78c80253d05542f3ec83b04e5df984858dcfab94"}, "cell_type": "code", "source": ["# Ent\u00e3o criamos duas listas de senten\u00e7as, um para trainamento e outra para teste\n", "train_tsents = tsents[100:]\n", "test_tsents = tsents[:100]\n", "\n", "print(f\"train_tsents: {len(train_tsents)}\")\n", "print(f\"test_tsents: {len(test_tsents)}\")"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "43f17caf-bee1-4cf1-b421-0f84c6568454", "_uuid": "93e6073dea65e68615c5d76c5e59b60dc90ee230"}, "cell_type": "code", "source": ["# Sabemos que \"n\" \u00e9 a tag mais comum, ent\u00e3o podemos settar uma tag como default que marque todas as\n", "# palavras como um substantivo e ver como isso performa\n", "# NOTA: Por conta das mudan\u00e7as na API do NLTK, o exemplo do livro precisa ser reescrito...\n", "from nltk import DefaultTagger\n", "tagger0 = DefaultTagger(\"n\")\n", "tagger0.evaluate(test_tsents)"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "fe9cbc6c-84b3-47ec-93e2-60509f230c9b", "_uuid": "794b8e9f1aeb00ef8f143bc636aa9f46b731c93c"}, "cell_type": "code", "source": ["# Evidentemente, 1 em 6 palavras s\u00e3o substantivos. Vamos melhorar isso ent\u00e3o treinando um taggeador do\n", "# tipo unigram\n", "from nltk import UnigramTagger\n", "tagger1 = UnigramTagger(train_tsents, backoff=tagger0)\n", "tagger1.evaluate(test_tsents)"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "fa2920e6-1402-452a-8a9e-84c5c70de56d", "_uuid": "c33bd4e079fb75ece388b02371774243c916a1b6"}, "cell_type": "code", "source": ["# E agora, para finalizar, um taggeador bigram\n", "from nltk import BigramTagger\n", "tagger2 = BigramTagger(train_tsents, backoff=tagger1)\n", "tagger2.evaluate(test_tsents)"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "b087e943-25a6-4286-8ad6-2ae211a34747", "_uuid": "04424d1e4d8f04d988e5e1ca6631365a00d0f9f1"}, "cell_type": "code", "source": ["# B\u00f4nus: Vamos fazer uma avalia\u00e7\u00e3o de acuracidade com TnT (Trigrams'n'Tag), que \u00e9 bom, mas ainda \u00e9 um\n", "# pouco menos performatico que o BigramTagger\n", "from nltk.tag import tnt\n", "tnt_pos_tagger = tnt.TnT()\n", "tnt_pos_tagger.train(train_tsents)\n", "tnt_pos_tagger.evaluate(test_tsents)"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "14a2129c-7a83-4a25-b5c4-3c087f28f89b", "_uuid": "5442415905ac3df745dc6b04a40e2385252dc4ce"}, "cell_type": "markdown", "source": ["## 3.3. Segmenta\u00e7\u00e3o de Senten\u00e7as"]}, {"metadata": {"_cell_guid": "ab17d614-4c37-4a62-8eba-b2dfa2052a9e", "_uuid": "b5390a7089903dc27d5cec11aabc6d34fbbca21b"}, "cell_type": "code", "source": ["# Punkt \u00e9 uma ferramenta de segumenta\u00e7\u00e3o de linguagem-neutra\n", "from nltk import data\n", "sent_tokenizer = data.load(\"tokenizers/punkt/portuguese.pickle\")\n", "raw_text = machado.raw(\"romance/marm05.txt\")\n", "sentences = sent_tokenizer.tokenize(raw_text)\n", "for sent in sentences[1000:1005]:\n", "    print(\"<<\", sent, \">>\")"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "61922073-7f5c-4621-83b9-7c40d58088b0", "_uuid": "9b8ed8c88ed5291d9b7abeae992c0dec6e425daf"}, "cell_type": "code", "source": ["# Um sentence tokenizer pode ser treinado para ser aplicado em outros textos. O Floresta Portuguese\n", "# Treebank contem uma senten\u00e7a por linha, ent\u00e3o vamos convenientemente us\u00e1-lo para criar dois textos,\n", "# um de treinamento e um de teste\n", "import os, nltk.test\n", "\n", "text = floresta.raw()\n", "lines = text.split('\\n')\n", "train_text = \" \".join(lines[20:])\n", "test_text = \" \".join(lines[:20])\n", "\n", "print(f\"total de linhas no arquivo: {len(lines)}\")\n", "print(f\"train_text: {len(train_text)} caracteres\")\n", "print(f\"test_text: {len(test_text)} caracteres\")\n", "\n", "# Agora, vamos treinar o seguimentador de senten\u00e7as, a.k.a. sentence tokenizer, e us\u00e1-lo em nosso\n", "# texto de teste\n", "from nltk import PunktSentenceTokenizer\n", "stok = PunktSentenceTokenizer(train_text)\n", "\n", "tok_text = stok.tokenize(test_text)\n", "print(f\"texto de teste segmentado: {len(tok_text)}\")\n", "tok_text"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "d3915905-4925-4b6b-96ad-026f0f91e457", "_uuid": "7795a523ad5a3ad275ff972d381baaa0f4e201f0"}, "cell_type": "code", "source": ["# NLTK j\u00e1 inclui um modelo treinado para a gera\u00e7\u00e3o de seten\u00e7as em portugu\u00eas, que pode ser carregado\n", "# a partir de um arquivo .pickle\n", "stok = data.load(\"tokenizers/punkt/portuguese.pickle\")\n", "stok"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "f1779015-9c6e-4ac6-8899-da4895a9280e", "_uuid": "19d12ce863b38e35c769f38eee5fa9df74bde845"}, "cell_type": "markdown", "source": ["## 3.4. Stemming"]}, {"metadata": {"_cell_guid": "709a4159-f5ea-40f5-8574-9fbbca4ffe47", "_uuid": "7f8986d2ea21315f9abf2686f3dba20486f486fe"}, "cell_type": "code", "source": ["# Stopwords do portugu\u00eas\n", "stopwords = nltk.corpus.stopwords.words('portuguese')\n", "stopwords[:10]"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "898d5119-ec76-497e-ae4b-2b64e8fea07a", "_uuid": "638e919e0c910e1854702021d13edf2e0e9ff882"}, "cell_type": "code", "source": ["# Podemos ent\u00e3o usar isso para filtrar texto\n", "words = [w.lower() for w in floresta.words() if w.lower() not in stopwords]\n", "fd = nltk.FreqDist(words)\n", "fd_keys = [x for x in fd.keys()]\n", "\n", "print(\"distribui\u00e7\u00e3o de frequ\u00eancia das palavras que n\u00e3o s\u00e3o stopwords:\")\n", "for word in fd_keys[:20]:\n", "    print(f\"{word} = {fd[word]}\")"], "execution_count": null, "outputs": []}]}